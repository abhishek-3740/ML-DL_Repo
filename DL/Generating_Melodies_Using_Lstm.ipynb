{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import music21 as m21\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import gc\n",
        "\n",
        "# ============================================================================\n",
        "# CHECKPOINT MANAGEMENT (Your original class)\n",
        "# ============================================================================\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"Manages model checkpoints.\"\"\"\n",
        "    def __init__(self, checkpoint_dir='checkpoints'):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, scheduler, scaler, epoch, loss, model_params, is_best=False):\n",
        "        \"\"\"Save checkpoint with all training state.\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "            'loss': loss,\n",
        "            'model_params': model_params,\n",
        "        }\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        latest_path = os.path.join(self.checkpoint_dir, 'checkpoint_latest.pth')\n",
        "        torch.save(checkpoint, latest_path)\n",
        "        print(f\"✓ Checkpoint saved: {latest_path}\")\n",
        "\n",
        "        if is_best:\n",
        "            best_path = os.path.join(self.checkpoint_dir, 'model_best.pth')\n",
        "            torch.save(checkpoint, best_path)\n",
        "            print(f\"✓ Best model saved: {best_path}\")\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load the latest checkpoint.\"\"\"\n",
        "        checkpoint_path = os.path.join(self.checkpoint_dir, 'checkpoint_latest.pth')\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
        "            return torch.load(checkpoint_path)\n",
        "        print(\"No checkpoint found, starting from scratch.\")\n",
        "        return None\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "# Unzip data if not already done\n",
        "if not os.path.exists('/content/essen'):\n",
        "    !unzip -q /content/deutschl.zip -d /content/\n",
        "\n",
        "KERN_DATASET_PATH = '/content/essen/europa/deutschl/test'\n",
        "PREPROCESSED_DATA_PATH = \"dataset\"\n",
        "SINGLE_FILE_DATASET_PATH = \"file_dataset\"\n",
        "MAPPING_PATH = \"mapping.json\"\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "\n",
        "# Preprocessing & Model Hyperparameters\n",
        "ACCEPTABLE_DURATIONS = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0]\n",
        "SEQUENCE_LENGTH = 64\n",
        "BATCH_SIZE = 128 # Increased for better GPU utilization\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_SIZES = [256, 256]\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(PREPROCESSED_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(SINGLE_FILE_DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPROCESSING (Your original functions)\n",
        "# ============================================================================\n",
        "def load_songs_in_kern(dataset_path):\n",
        "    songs = []\n",
        "    for path, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".krn\"):\n",
        "                try:\n",
        "                    song = m21.converter.parse(os.path.join(path, file))\n",
        "                    songs.append(song)\n",
        "                except m21.converter.ConverterException:\n",
        "                    print(f\"Warning: Could not parse {file}. Skipping.\")\n",
        "    return songs\n",
        "\n",
        "def has_acceptable_durations(song, acceptable_durations):\n",
        "    for note in song.flat.notesAndRests:\n",
        "        if note.duration.quarterLength not in acceptable_durations:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def transpose(song):\n",
        "    try:\n",
        "        parts = song.getElementsByClass(m21.stream.Part)\n",
        "        measures = parts[0].getElementsByClass(m21.stream.Measure)\n",
        "        key = measures[0][4]\n",
        "    except (IndexError, AttributeError):\n",
        "        key = None\n",
        "    if not isinstance(key, m21.key.Key):\n",
        "        key = song.analyze(\"key\")\n",
        "    print(f\"Transposing from key: {key}\")\n",
        "    mode = key.mode if key.mode in [\"major\", \"minor\"] else \"major\"\n",
        "    if mode == \"major\":\n",
        "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"C\"))\n",
        "    else:\n",
        "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"A\"))\n",
        "    return song.transpose(interval)\n",
        "\n",
        "def encode_song(song):\n",
        "    encoded_song = []\n",
        "    for event in song.flat.notesAndRests:\n",
        "        symbol = \"r\" if isinstance(event, m21.note.Rest) else event.pitch.midi\n",
        "        steps = int(event.duration.quarterLength / 0.25)\n",
        "        for step in range(steps):\n",
        "            encoded_song.append(str(symbol) if step == 0 else \"_\")\n",
        "    return \" \".join(encoded_song)\n",
        "\n",
        "def preprocess(dataset_path):\n",
        "    print('Loading songs....')\n",
        "    songs = load_songs_in_kern(dataset_path)\n",
        "    print(f'Loaded {len(songs)} songs.')\n",
        "    saved_count = 0\n",
        "    for i, song in enumerate(songs):\n",
        "        if not has_acceptable_durations(song, ACCEPTABLE_DURATIONS):\n",
        "            continue\n",
        "        song = transpose(song)\n",
        "        encoded_song = encode_song(song)\n",
        "        save_path = os.path.join(PREPROCESSED_DATA_PATH, f\"{i}.txt\")\n",
        "        with open(save_path, \"w\") as fp:\n",
        "            fp.write(encoded_song)\n",
        "        saved_count += 1\n",
        "    print(f\"Preprocessed and saved {saved_count} songs.\")\n",
        "\n",
        "def create_single_file_dataset(dataset_path, file_dataset_path, sequence_length):\n",
        "    new_song_delimiter = \"/ \" * sequence_length\n",
        "    songs = \"\"\n",
        "    for path, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(path, file)\n",
        "            with open(file_path, \"r\") as fp:\n",
        "                song = fp.read()\n",
        "            songs += song + \" \" + new_song_delimiter\n",
        "    songs = songs.strip()\n",
        "    output_file_path = os.path.join(file_dataset_path, \"encoded_songs.txt\")\n",
        "    with open(output_file_path, \"w\") as fp:\n",
        "        fp.write(songs)\n",
        "    return songs\n",
        "\n",
        "def create_mapping(songs, mapping_path):\n",
        "    songs = songs.split()\n",
        "    vocabulary = sorted(list(set(songs)))\n",
        "    mappings = {symbol: i for i, symbol in enumerate(vocabulary)}\n",
        "    with open(mapping_path, \"w\") as fp:\n",
        "        json.dump(mappings, fp, indent=4)\n",
        "    print(\"Vocabulary mapping created.\")\n",
        "    return mappings\n",
        "\n",
        "# ============================================================================\n",
        "# PYTORCH DATASET AND MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class MusicDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for loading music sequences.\"\"\"\n",
        "    def __init__(self, int_songs, sequence_length):\n",
        "        self.int_songs = int_songs\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.int_songs) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        inputs = torch.tensor(self.int_songs[index:index+self.sequence_length], dtype=torch.long)\n",
        "        target = torch.tensor(self.int_songs[index+self.sequence_length], dtype=torch.long)\n",
        "        return inputs, target\n",
        "\n",
        "class MusicLSTM(nn.Module):\n",
        "    \"\"\"LSTM model for music generation.\"\"\"\n",
        "    def __init__(self, vocabulary_size, embedding_dim, hidden_sizes, dropout_rate):\n",
        "        super(MusicLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "        # Using a multi-layer LSTM is more conventional in PyTorch\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_sizes[0], num_layers=2,\n",
        "                            batch_first=True, dropout=dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_sizes[0], vocabulary_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # We only need the output of the last time step\n",
        "        out = lstm_out[:, -1, :]\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "def train():\n",
        "    \"\"\"Main training loop.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- 1. Load Data and Mappings ---\n",
        "    with open(MAPPING_PATH, \"r\") as fp:\n",
        "        mappings = json.load(fp)\n",
        "    vocabulary_size = len(mappings)\n",
        "\n",
        "    encoded_songs_path = os.path.join(SINGLE_FILE_DATASET_PATH, \"encoded_songs.txt\")\n",
        "    with open(encoded_songs_path, \"r\") as fp:\n",
        "        songs = fp.read()\n",
        "\n",
        "    int_songs = [mappings[symbol] for symbol in songs.split()]\n",
        "\n",
        "    # --- 2. Create Dataset and DataLoader ---\n",
        "    dataset = MusicDataset(int_songs, SEQUENCE_LENGTH)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # --- 3. Initialize Model and Training Components ---\n",
        "    model_params = {\n",
        "        'vocabulary_size': vocabulary_size,\n",
        "        'embedding_dim': EMBEDDING_DIM,\n",
        "        'hidden_sizes': HIDDEN_SIZES,\n",
        "        'dropout_rate': DROPOUT_RATE\n",
        "    }\n",
        "    model = MusicLSTM(**model_params).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5) # Learning rate decay\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scaler = GradScaler()\n",
        "    checkpoint_manager = CheckpointManager(CHECKPOINT_DIR)\n",
        "\n",
        "    # --- 4. Load from Checkpoint if available ---\n",
        "    start_epoch = 0\n",
        "    best_loss = float('inf')\n",
        "    checkpoint = checkpoint_manager.load_checkpoint()\n",
        "    if checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_loss = checkpoint.get('loss', float('inf'))\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    # --- 5. Training Loop ---\n",
        "    model.train()\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_predictions += targets.size(0)\n",
        "            correct_predictions += (predicted == targets).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "        print(f\"--- End of Epoch [{epoch+1}/{EPOCHS}], Average Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f} ---\")\n",
        "\n",
        "        # --- 6. Save Checkpoint ---\n",
        "        is_best = epoch_loss < best_loss\n",
        "        if is_best:\n",
        "            best_loss = epoch_loss\n",
        "\n",
        "        checkpoint_manager.save_checkpoint(\n",
        "            model=model, optimizer=optimizer, scheduler=scheduler, scaler=scaler,\n",
        "            epoch=epoch, loss=epoch_loss, model_params=model_params, is_best=is_best\n",
        "        )\n",
        "        scheduler.step()\n",
        "        gc.collect() # Garbage collection\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run preprocessing steps\n",
        "    preprocess(KERN_DATASET_PATH)\n",
        "    songs_str = create_single_file_dataset(PREPROCESSED_DATA_PATH, SINGLE_FILE_DATASET_PATH, SEQUENCE_LENGTH)\n",
        "    create_mapping(songs_str, MAPPING_PATH)\n",
        "\n",
        "    # Start training\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oEd5_xZGIKn",
        "outputId": "d5355880-5957-49eb-856d-eaac2297e964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading songs....\n",
            "Loaded 12 songs.\n",
            "Transposing from key: F major\n",
            "Transposing from key: e minor\n",
            "Transposing from key: F major\n",
            "Transposing from key: C major\n",
            "Transposing from key: b minor\n",
            "Transposing from key: e minor\n",
            "Transposing from key: e minor\n",
            "Transposing from key: C major\n",
            "Transposing from key: e minor\n",
            "Transposing from key: g minor\n",
            "Transposing from key: F major\n",
            "Transposing from key: C major\n",
            "Preprocessed and saved 12 songs.\n",
            "Vocabulary mapping created.\n",
            "Using device: cuda\n",
            "Loading checkpoint from: checkpoints/checkpoint_latest.pth\n",
            "Resuming training from epoch 5\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/music21/stream/base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
            "  return self.iter().getElementsByClass(classFilterList)\n",
            "/tmp/ipython-input-1396220947.py:242: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441b11b4",
        "outputId": "f00879f9-13e4-4b2f-d875-69d5cf8a6bdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Assuming the necessary classes and functions (MusicDataset, MusicLSTM, etc.) are defined in the previous cell\n",
        "\n",
        "# Load the mapping\n",
        "MAPPING_PATH = \"mapping.json\"\n",
        "with open(MAPPING_PATH, \"r\") as fp:\n",
        "    mappings = json.load(fp)\n",
        "vocabulary_size = len(mappings)\n",
        "\n",
        "# Load the preprocessed integer songs\n",
        "SINGLE_FILE_DATASET_PATH = \"file_dataset\"\n",
        "encoded_songs_path = os.path.join(SINGLE_FILE_DATASET_PATH, \"encoded_songs.txt\")\n",
        "with open(encoded_songs_path, \"r\") as fp:\n",
        "    songs = fp.read()\n",
        "int_songs = [mappings[symbol] for symbol in songs.split()]\n",
        "\n",
        "# Create the dataset and dataloader for evaluation\n",
        "SEQUENCE_LENGTH = 64\n",
        "BATCH_SIZE = 128 # Use the same batch size or adjust as needed\n",
        "dataset = MusicDataset(int_songs, SEQUENCE_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True) # Shuffle should be False for evaluation\n",
        "\n",
        "# Load the best model checkpoint\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "best_model_path = os.path.join(CHECKPOINT_DIR, 'model_best.pth')\n",
        "\n",
        "if not os.path.exists(best_model_path):\n",
        "    print(f\"Error: Best model checkpoint not found at {best_model_path}\")\n",
        "else:\n",
        "    print(f\"Loading best model from: {best_model_path}\")\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "\n",
        "    # Re-initialize the model with the saved parameters\n",
        "    model_params = checkpoint['model_params']\n",
        "    model = MusicLSTM(**model_params)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Set model to evaluation mode and move to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Calculating accuracy...\")\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad(): # No gradient calculation needed for evaluation\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_predictions += targets.size(0)\n",
        "            correct_predictions += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Accuracy of the best model on the training dataset: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model from: checkpoints/model_best.pth\n",
            "Calculating accuracy...\n",
            "Accuracy of the best model on the training dataset: 0.7715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xFBPzFfvHpxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}